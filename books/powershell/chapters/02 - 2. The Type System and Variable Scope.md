# 2. The Type System and Variable Scope

PowerShell's type system is unique among shells: it combines the convenience of dynamic typing with the power and safety of .NET's strongly-typed runtime. Unlike Bash, where everything is text until you parse it into something else, PowerShell operates on rich .NET objects with well-defined types. Unlike Python, where types are discovered purely at runtime, PowerShell lets you optionally constrain variables to specific types when correctness matters more than flexibility.

This chapter explores how PowerShell stores and manages data in memory, how variables work across different scopes, and how to choose the right collection types and constraints for your scripts. Understanding these concepts is essential for writing efficient, maintainable automation that scales from quick one-liners to production modules.

The Extended Type System (ETS) sits between your PowerShell code and the underlying .NET types, smoothing over rough edges and adding conveniences like NoteProperties and ScriptProperties without requiring you to write C#. This layer is what makes PowerShell feel dynamic and approachable while still giving you access to the full power of statically typed objects when you need them.

## Prereqs

- Chapter 1 concepts: object-first thinking, discovery cmdlets, and basic command syntax
- A working PowerShell session (`powershell.exe` or `pwsh`)
- Comfort running local, read-only commands such as `Get-Process`

## Learning Outcomes

- Explain how variables reference .NET objects in memory
- Choose when to use dynamic typing vs. type constraints
- Use splatting, hashtables, and generic lists effectively
- Avoid common collection performance pitfalls (such as `+=` in loops)
- Predict how scope affects variable visibility and lifetime
- Apply closures and scope modifiers intentionally

## Key Terms

ETS, type accelerator, cast, splatting, hashtable, generic list, scope, closure, dot-sourcing

## Example Labels

- `<!-- Illustrative -->`: concept-focused examples that may need adaptation for your environment
- `<!-- Tested on: ... -->`: examples validated on a specific PowerShell/runtime version

## 2.1 Variables and Assignment

At the most fundamental level, PowerShell variables are references to .NET objects stored in managed memory. When you write `$name = "Alice"`, you are not storing the string directly "inside" the variable; instead, you create a System.String object on the heap and point a named reference at it. This distinction matters when you pass variables to functions, compare them, or try to understand why changing a variable in one scope might not affect another.

The dollar sign (`$`) is a **sigil** (a symbol attached to a variable name to identify it as such) that marks the beginning of a variable reference. Unlike languages such as C# where variable names stand alone, PowerShell requires the sigil so that the parser can unambiguously distinguish between a command name, a parameter, and a variable during tokenization. This makes PowerShell's grammar simpler and more predictable, especially when variables appear inline with other tokens.

Assignment is straightforward:

<!-- Illustrative -->
```powershell
$count = 10
$message = "Hello, World"
$processes = Get-Process
```

Each assignment updates the reference held by that variable name in the current scope. If the variable did not exist, it is created; if it already exists, its reference is updated to point to the new object. The old object remains in memory until the garbage collector determines it is no longer reachable.

### 2.1.1 Naming Rules and Sigils ($)

PowerShell variable names are more flexible than many other languages. Unbraced variable names follow identifier-like rules (letters, digits, and underscores; Unicode letters are also allowed). By convention, PowerShell developers use camelCase for local variables (`$myVariable`) and PascalCase for parameters and function names, but the engine itself is case-insensitive and allows a broad set of names.

What makes PowerShell unusual is that you can *escape* the normal name-parsing rules using brace notation. When you wrap a variable name in `${...}`, PowerShell treats everything inside the braces as the variable name. In practice, this is most useful for disambiguating variable expansion inside strings and handling unusual names (for example, environment-style names that contain parentheses or punctuation). Most of the time, sticking to simple identifier-like names keeps your code readable and maintainable.

#### 2.1.1.1 Valid Character Sets

PowerShell does not enforce case sensitivity for variable names. `$Count`, `$count`, and `$COUNT` all refer to the same variable. While this flexibility can be convenient at the prompt, it can also introduce subtle bugs in scripts if you are not consistent. Most style guides recommend choosing one casing convention and sticking with it throughout your code:

| Convention     | Example          | Recommended Use                    |
|----------------|------------------|------------------------------------|
| camelCase      | `$userName`      | Local variables, private members   |
| PascalCase     | `$ConnectionString` | Parameters, exported functions  |
| snake_case     | `$max_retries`   | Rare; used for readability in some teams |
| SCREAMING_CASE | `$API_KEY`       | Constants (with `Set-Variable -Option Constant`) |

**2.1.1.1.1 Alphanumeric and Underscores**

Standard variable names look and behave like variables in most modern languages:

```powershell
$userName = "Alice"
$item_count = 42
$result2 = Get-Process
```

These names are easy to tab-complete, easy to search for, and carry clear intent. Using underscores or mixed case helps separate words when the name spans multiple concepts (for example, `$max_retry_count` or `$maxRetryCount`).

Leading digits are a common gotcha. PowerShell does not treat `$2ndVariable` as a normal unbraced variable name; the parser interprets `$2` as a valid variable reference (often used for regex capture groups) and treats the remaining text `ndVariable` as separate literal characters. If you truly need a name that starts with a digit, use braces:

```powershell
${2ndVariable} = "valid with braces"
"${2ndVariable}"   # Expands correctly
"$2ndVariable"     # Expands $2, then appends 'ndVariable' (common gotcha)
```

**2.1.1.1.2 Escaping special characters with braces ${...}**

When you need a variable name that violates the standard character rules, wrap the entire name in braces immediately after the dollar sign:

```powershell
${My Variable With Spaces} = "Valid, but awkward"
${User@Domain.com} = "Also valid"
${Path\To\Resource} = "Edge case for providers"
```

Brace notation is especially helpful when you need to disambiguate variable expansion inside double-quoted strings:

```powershell
$name = "Alice"
"User: ${name}Suffix"   # Unambiguous
"User: $nameSuffix"     # Looks for $nameSuffix
```

It also works for unusual names that include spaces or punctuation. In normal script code, however, frequent use of brace-escaped names is a sign that you should rethink your naming strategy and choose simpler identifiers instead.

#### 2.1.1.2 The Sigil's Role

The dollar sign serves two critical purposes in PowerShell. First, it acts as a delimiter that tells the parser "what follows is a variable name, not a cmdlet or keyword." Second, it enables variable interpolation inside double-quoted strings without requiring additional syntax.

Because the sigil is mandatory, you never have ambiguity between a bare word and a variable. In languages without sigils, parsers must maintain complex context about whether `foo` is a variable, a function call, or a string literal. PowerShell avoids this by requiring `$foo` for variables, making both human readers and the parser's job simpler.

**2.1.1.2.1 Variable interpolation trigger**

When the tokenizer encounters a dollar sign, it begins scanning for a variable reference. In a double-quoted string, that is what triggers variable interpolation. (String quoting and interpolation are covered in detail in §2.2.1.)

For complex expressions inside strings, use the subexpression operator `$(...)`:

```powershell
$process = Get-Process -Id $PID
$message = "PowerShell is using $($process.WorkingSet / 1MB) MB of RAM."
```

This evaluates the expression `$process.WorkingSet / 1MB` and inserts the result into the string, demonstrating how the sigil and subexpression syntax work together.

**2.1.1.2.2 Distinguishing variable value from object reference**

It is important to distinguish between `$var` and `variable:var`. The former retrieves the *value* stored in the variable, while the latter refers to the variable itself as an item in the `Variable:` provider drive.

Most of the time you work with values:

```powershell
$count = 10
Write-Output $count   # Outputs 10
```

Occasionally you need to manipulate the variable container itself, for example to inspect metadata or remove it:

```powershell
Get-Item variable:count
Remove-Item variable:count
```

This provider-based view is what makes it possible to treat variables as "files" in a virtual drive, which we will revisit later in this chapter. The following table clarifies the distinction:

| Syntax          | What It Refers To                        | Common Use                        |
|-----------------|------------------------------------------|-----------------------------------|
| `$var`          | The value stored in the variable         | Reading, assigning, passing data  |
| `variable:var`  | The variable container itself (metadata) | Inspecting options, removing vars |
| `Get-Variable var` | Cmdlet retrieval returning variable object | Advanced scenarios, scope inspection |

### 2.1.2 Explicit vs. Implicit Typing

PowerShell is dynamically typed by default, meaning that variables adapt to the type of data you assign to them. Assign an integer and the variable holds an `Int32`; assign a string and it becomes a `System.String`. This flexibility makes the shell comfortable for interactive work, where you rarely want to think about type declarations.

At the same time, PowerShell supports explicit type constraints. When you need guarantees—when you want to ensure that a parameter always receives an integer, for example, or that a configuration variable never accidentally becomes a string—you can lock a variable to a specific type. The engine will then enforce that constraint for the lifetime of the variable, performing automatic conversions where possible and raising errors when conversion fails.

This hybrid model gives you the best of both worlds: the speed and ease of dynamic typing when you are exploring, and the safety and clarity of strong typing when you are building production code.

#### 2.1.2.1 Dynamic Typing (Sometimes Loosely Called "Duck Typing")

By default, PowerShell does not require you to declare types. The runtime inspects the value you assign and sets the variable's type accordingly:

```powershell
$data = 42             # $data is [int]
$data.GetType().FullName   # System.Int32

$data = "Hello"        # Now $data is [string]
$data.GetType().FullName   # System.String
```

This flexibility is sometimes *loosely* called "duck typing," but more precisely this example demonstrates dynamic typing: the variable can hold different types over time. Strictly speaking, duck typing refers to using an object based on the members it exposes (what it can do), regardless of its declared type.

**2.1.2.1.1 Runtime type determination**

Each time you assign a value, the PowerShell engine inspects the payload to determine its .NET type. For literals, this is straightforward: `10` is an `Int32`, `10.5` is a `Double`, `"text"` is a `String`. For objects returned from cmdlets or methods, the type is whatever the underlying .NET code produced.

You can inspect the current type at any time with `.GetType()`:

```powershell
$result = Get-Process -Id $PID
$result.GetType().FullName   # System.Diagnostics.Process
```

Understanding what type you are working with helps you predict which properties and methods are available, and it is the first step in debugging type-related errors.

Note: If you project specific properties with `Select-Object Name, CPU`, the result is a `System.Management.Automation.PSCustomObject` with **NoteProperties** added via PowerShell's Extended Type System (ETS). These are dynamically attached properties that behave like regular properties but are stored separately from the object's native .NET type definition.

**2.1.2.1.2 Automatic type widening (Int to Double)**

When you perform operations that mix types, PowerShell automatically widens narrower types to accommodate the operation. For example, adding an integer and a double produces a double:

```powershell
$a = 10      # Int32
$b = 3.5     # Double
$c = $a + $b # $c is Double (13.5)
```

This widening happens transparently to prevent data loss. Common widening scenarios include:

| Operation          | Left Type | Right Type | Result Type | Notes                              |
|--------------------|-----------|------------|-------------|------------------------------------|
| `10 + 3.5`         | Int32     | Double     | Double      | Integer promoted to floating-point |
| `10 + 10000000000L`| Int32     | Int64      | Int64       | 32-bit promoted to 64-bit          |
| `10 + 19.99D`      | Int32     | Decimal    | Decimal     | Promoted to highest precision      |
| `"10" + 5`         | String    | Int32      | String      | Numeric converted to string (concatenation) |

The takeaway is that PowerShell tries to do the "right thing" with mixed-type arithmetic, but you should remain aware of the types involved if precision or performance matters.

#### 2.1.2.2 Constrained Variables (Type Constraints)

When you prefix a variable assignment with a type cast, PowerShell constrains that variable to the specified type for its entire lifetime in that scope (also commonly called "type locking" or creating a "strongly typed variable"). Any subsequent assignment must be convertible to that type, or the engine raises an error.

Type-constrained variables are essential when correctness is more important than flexibility. In function parameters, for example, you typically constrain types to ensure callers pass valid data. In configuration scripts, you might lock critical variables to prevent accidental corruption.

**2.1.2.2.1 Casting on the left side of assignment**

The syntax for type-locking is straightforward: place the type name in square brackets before the variable name on the first assignment:

```powershell
[int]$count = 10
$count = 20        # Works: 20 is an int
$count = "Hello"   # Fails: cannot convert "Hello" to int
```

Once `$count` is constrained to `[int]`, every assignment goes through the type converter. If the converter succeeds, the new value is stored; if it fails, you get a clear error message explaining the type mismatch.

This locking persists for the variable's lifetime in that scope, even across assignments:

```powershell
[string]$message = "Initial"
$message = 42        # Works: 42 is converted to the string "42"
$message.GetType().FullName   # System.String
```

Here, the integer `42` was automatically converted to a string to satisfy the constraint.

**2.1.2.2.2 Type conversion exceptions**

When the type converter cannot perform a requested conversion, PowerShell raises an error immediately rather than silently truncating or mis-representing data:

```powershell
[int]$number = "NotANumber"
# Error: Cannot convert value "NotANumber" to type "System.Int32"
```

These errors are your friend: they surface bugs at the point of assignment instead of letting invalid data propagate through your script. In production code, consider using `try/catch` blocks around risky conversions and handle failures explicitly.

### 2.1.3 Variable Providers (Env:, Variable:)

PowerShell exposes several system concepts—including variables themselves—as virtual drives using the Provider model. The `Variable:` drive lets you treat variables as items you can list, copy, or delete just like files. The `Env:` drive provides similar access to environment variables, bridging PowerShell and the operating system's environment block.

This abstraction is more than a curiosity; it enables powerful meta-programming scenarios and gives you a uniform way to inspect or manipulate state that would otherwise require special cmdlets or .NET calls.

#### 2.1.3.1 The PSDrive Abstraction

Every PowerShell session includes a set of built-in drives. You can see them all with `Get-PSDrive`:

```powershell
Get-PSDrive | Select-Object Name, Provider, Root
```

Among these, the `Variable:` drive represents the current session's variable table. Each variable in scope appears as an "item" you can query or manipulate with standard item cmdlets.

**2.1.3.1.1 Treating variables as files**

Because variables are exposed as items, you can list them like you would list files:

```powershell
Get-ChildItem variable: | Select-Object Name, Value
```

This command shows every variable in the current scope, along with its value. It is an invaluable diagnostic tool when you want to see what state exists in your session, especially after importing modules or dot-sourcing scripts.

You can also filter by name:

```powershell
Get-ChildItem variable:PS* | Format-Table Name, Value
```

This retrieves all variables whose names start with "PS", such as `$PSVersionTable`, `$PSModulePath`, and other built-in automatic variables.

**2.1.3.1.2 Copy-Item and Remove-Item on variables**

The provider model extends to modification as well. You can copy variables:

```powershell
$original = "Value"
Copy-Item variable:original variable:copy
$copy   # Outputs "Value"
```

Or remove them explicitly without using `Remove-Variable`:

```powershell
Remove-Item variable:copy
$copy   # Now undefined or $null
```

While `Remove-Variable` and `Remove-Item variable:...` are functionally equivalent, the provider syntax makes it clear that you are operating on the variable container itself, not just setting a value to `$null`.

#### 2.1.3.2 Environment Variable Persistence

The `Env:` drive provides access to environment variables—the key-value pairs that the operating system uses to configure processes. Unlike regular PowerShell variables, which exist only in memory for the session's lifetime, environment variables can persist across sessions and even across reboots depending on their scope.

When you reference `$env:PATH` or `$env:TEMP`, you are reading from this provider drive. Changes you make in PowerShell affect only the current process by default; to make them permanent, you must explicitly update the user or machine environment through .NET or registry APIs.

**2.1.3.2.1 Process vs. User vs. Machine targets**

Environment variables exist at three distinct scopes:

- **Process**: Temporary variables that live only for the current PowerShell session. When you set `$env:MyVar = "Value"` directly, you are modifying the process scope.
- **User**: Persistent variables stored in the current user's profile (Windows registry or user shell configuration files). These apply to all new processes started by that user.
- **Machine**: System-wide variables stored in machine-level configuration. These apply to all users and require administrative privileges to modify.

When you read `$env:PATH`, you see the *merged* result of all three scopes: the machine PATH, the user PATH, and any process-specific additions. The following table summarizes their characteristics:

| Scope    | Persistence        | Access Level | Registry Location (Windows)                  | Typical Use                   |
|----------|--------------------|--------------|----------------------------------------------|-------------------------------|
| Process  | Current session    | Current user | (none - in-memory only)                      | Temporary overrides           |
| User     | Survives reboot    | Current user | `HKCU:\Environment`                          | User-specific tools, paths    |
| Machine  | Survives reboot    | All users    | `HKLM:\SYSTEM\CurrentControlSet\Control\Session Manager\Environment` | System-wide paths, shared settings |

**2.1.3.2.2 \[Environment\]::SetEnvironmentVariable usage**

To make persistent changes to environment variables, use the .NET `[Environment]` class:

```powershell
# Set a user-level environment variable
[Environment]::SetEnvironmentVariable('MY_VAR', 'PersistentValue', 'User')

# Set a machine-level variable (requires admin)
[Environment]::SetEnvironmentVariable('COMPANY_ROOT', 'C:\Company', 'Machine')
```

The third parameter specifies the target scope. Changes made this way persist across sessions and reboots. Note that modifying machine-level variables requires running PowerShell as an administrator.

To retrieve a value from a specific scope:

```powershell
[Environment]::GetEnvironmentVariable('PATH', 'User')
[Environment]::GetEnvironmentVariable('PATH', 'Machine')
```

This lets you inspect each layer separately before the operating system merges them into the final `$env:PATH` you see in your process.

On non-Windows platforms, the `Env:` drive still represents the current process environment. The Windows-style `User` and `Machine` persistence targets are not a general cross-platform persistence model in the way they are on Windows. To persist values for future sessions, update the appropriate shell profile or OS configuration (for example, profile files or `/etc/environment`) and restart affected processes so they read the new environment.

### 2.1.4 Splatting: Passing Hashtables to Parameters

Splatting is one of PowerShell's most underappreciated features. Instead of writing long, unwieldy command lines with many parameters, you can define those parameters in a hashtable and then "explode" the hashtable into the command. This dramatically improves readability, makes it easier to reuse parameter sets, and allows you to construct parameters dynamically based on logic.

The key is the `@` symbol. When you define a hashtable with `$`, you are working with it as data. When you pass it to a command with `@`, you are instructing PowerShell to treat each key as a parameter name and each value as the argument for that parameter.

#### 2.1.4.1 The @ Syntax for Splatting

Here is the basic syntax:

```powershell
$params = @{
    Path        = 'C:\Logs'
    Recurse     = $true
    File        = $true
    ErrorAction = 'SilentlyContinue'
}

Get-ChildItem @params
```

This is equivalent to:

```powershell
Get-ChildItem -Path 'C:\Logs' -Recurse -File -ErrorAction SilentlyContinue
```

The splatted version is easier to read, easier to modify (you can comment out individual keys), and easier to build programmatically.

**2.1.4.1.1 Parameter binding handling of @var vs $var**

When PowerShell invokes a command, parameter binding recognizes `@params` in argument position as a splatting operation. It retrieves the hashtable stored in `$params`, enumerates the keys, and binds each key-value pair to the cmdlet's parameters as if you had typed them explicitly.

This transformation happens during the parameter-binding phase, before the cmdlet executes. From the cmdlet's perspective, there is no difference between receiving parameters via splatting and receiving them directly on the command line.

Because parameter binding performs this expansion, you can mix splatted parameters with regular parameters:

```powershell
Get-ChildItem @params -Filter '*.log'
```

Here, the `-Filter` parameter is added explicitly, while the others come from the hashtable.

**2.1.4.1.2 Array-based splatting for positional args**

In addition to hashtables, you can splat arrays to provide positional arguments. Keep this pattern to positional *values* (not switch names encoded as strings):

```powershell
$positionalArgs = 'C:\Logs', '*.txt'
Get-ChildItem @positionalArgs
```

This form is less common because it relies on positional binding, which can be fragile if parameter order changes. Also avoid naming your variable `$args`: `$args` is an automatic variable inside functions and script blocks. Hashtable splatting is almost always preferable for readability and robustness, especially when switches are involved.

#### 2.1.4.2 Benefits of Splatting

Splatting is not just syntactic sugar; it enables patterns that would be cumbersome or impossible with traditional parameter syntax. Long parameter lists become manageable, conditional logic can determine which parameters to include, and you can reuse parameter sets across multiple commands without duplicating code.

Key benefits of splatting include:

- **Readability**: Each parameter on its own line makes intent clear
- **Maintainability**: Easy to add, remove, or comment out parameters
- **Version control friendly**: Parameter changes create clean, focused diffs
- **Dynamic construction**: Build parameter sets conditionally with `if` statements
- **Reusability**: Define parameter sets once, use them across multiple commands
- **Debugging**: Inspect `$params` hashtable to see exactly what will be passed

**2.1.4.2.1 Code readability and formatting**

Compare a traditional invocation:

```powershell
New-ADUser -Name "John Doe" -GivenName "John" -Surname "Doe" -UserPrincipalName "john.doe@contoso.com" -Path "OU=Users,DC=contoso,DC=com" -AccountPassword (ConvertTo-SecureString "P@ssw0rd" -AsPlainText -Force) -Enabled $true
```

With the splatted equivalent:

```powershell
$userParams = @{
    Name              = "John Doe"
    GivenName         = "John"
    Surname           = "Doe"
    UserPrincipalName = "john.doe@contoso.com"
    Path              = "OU=Users,DC=contoso,DC=com"
    AccountPassword   = ConvertTo-SecureString "P@ssw0rd" -AsPlainText -Force
    Enabled           = $true
}

New-ADUser @userParams
```

The splatted version is easier to read, easier to modify, and easier to review in source control. Each parameter occupies its own line, making diffs clean and reducing the risk of introducing errors when you add or remove parameters.

**2.1.4.2.2 Dynamic parameter construction**

Splatting shines when you need to construct parameters programmatically:

```powershell
$params = @{
    Path = 'C:\Logs'
}

if ($Recurse) {
    $params['Recurse'] = $true
}

if ($Filter) {
    $params['Filter'] = $Filter
}

Get-ChildItem @params
```

This pattern lets you build up a parameter set based on conditions, user input, or configuration files. It is far cleaner than trying to construct a command string and then invoking it with `Invoke-Expression`, which is both error-prone and a security risk.

## 2.2 The Primitive Types

PowerShell's primitive types are the .NET types you encounter most frequently in day-to-day scripting: strings, numbers, booleans, and dates. Understanding how these types behave—how they are created, converted, and compared—is foundational to writing correct, efficient scripts.

Unlike some interpreted languages where "everything is a string," PowerShell preserves type information throughout the pipeline. A string is a `System.String`, an integer is `System.Int32`, and a date is `System.DateTime`. This fidelity gives you predictable behavior and allows the runtime to optimize operations based on actual types.

### 2.2.1 Strings: Literal (' ') vs. Expandable (" ")

Strings are perhaps the most frequently used type in any shell. PowerShell provides two syntaxes for defining strings, each with different rules for variable expansion and escape sequences. Choosing the right one depends on whether you want the parser to interpolate variables and evaluate expressions inside the string.

Single-quoted strings are literal: what you type is what you get. Double-quoted strings are expandable: the parser scans for variables and subexpressions, replacing them with their values before the string is created.

#### 2.2.1.1 Variable Interpolation Logic

Double-quoted strings enable variable interpolation:

```powershell
$name = "Alice"
$greeting = "Hello, $name!"
Write-Output $greeting   # Outputs "Hello, Alice!"
```

The parser replaced `$name` with its value during string construction. Single quotes prevent this:

```powershell
$greeting = 'Hello, $name!'
Write-Output $greeting   # Outputs "Hello, $name!"
```

This distinction is critical when you are building strings that contain literal dollar signs, such as paths or script fragments.

**2.2.1.1.1 Sub-expression expansion $() inside strings**

For anything more complex than a simple variable reference, use the subexpression operator `$(...)`:

```powershell
$process = Get-Process -Id $PID
$message = "PowerShell is using $($process.WorkingSet / 1MB) MB of RAM."
Write-Output $message
```

The subexpression evaluates the expression `$process.WorkingSet / 1MB` and inserts the result into the string. Without the subexpression syntax, the parser would only expand `$process` and then append the literal text `.WorkingSet / 1MB`.

**2.2.1.1.2 Escaping the Sigil (Backtick usage)**

To include a literal dollar sign in a double-quoted string, escape it with a backtick:

```powershell
$price = 10
$message = "The item costs `$${price}."
Write-Output $message   # Outputs "The item costs $10."
```

The backtick (`` ` ``) is PowerShell's escape character. It also escapes other special characters such as `` `n `` (newline), `` `t `` (tab), and `` `r `` (carriage return).

Common escape sequences include:

| Sequence | Meaning                    | Example Usage                              |
|----------|----------------------------|--------------------------------------------|
| `` `n `` | Newline (line feed)        | `"Line1`nLine2"`                          |
| `` `r `` | Carriage return            | `"Col1`r`nCol2"` (Windows-style CRLF)    |
| `` `t `` | Tab character              | `"Name:`tValue"`                          |
| `` `$ `` | Literal dollar sign        | `"Price: `$19.99"`                        |
| `` `" `` | Literal double quote       | `"She said `"Hello`""`                    |
| backtick (`` ` ``) | Literal backtick   | Use a doubled backtick escape inside a double-quoted string |
| `` `0 `` | Null character             | Rarely used outside binary data           |

#### 2.2.1.2 String Interning and Immutability

Strings in .NET are immutable: once created, their content cannot change. When you "modify" a string—appending text, replacing characters, or changing case—you are actually creating a new string object and discarding the old one. This design simplifies thread safety and enables optimizations like string interning, but it has performance implications when you build strings incrementally.

Understanding immutability helps explain why certain string operations are slow and how to optimize them when performance matters.

**2.2.1.2.1 Memory footprint of duplicate strings**

When you create a string literal, the runtime may check whether an identical string already exists in the intern pool. If so, it reuses the existing instance rather than allocating new memory. This optimization, called *string interning*, reduces memory consumption when many variables reference the same text:

```powershell
$a = "Hello"
$b = "Hello"
[Object]::ReferenceEquals($a, $b)   # May return $true
```

Interning is most effective for literals and constants. Dynamically constructed strings are not automatically interned, so you may have many copies of the same text in memory if you build strings in loops.

**2.2.1.2.2 Performance cost of concatenation**

Because strings are immutable, concatenating them with `+=` creates a new string each time:

```powershell
$result = ""
foreach ($i in 1..1000) {
    $result += "Line $i`n"
}
```

This code creates 1,000 intermediate string objects, each slightly larger than the last, and discards 999 of them. The performance degrades quadratically as the string grows.

For large-scale string building, use `[System.Text.StringBuilder]`:

```powershell
$sb = [System.Text.StringBuilder]::new()
foreach ($i in 1..1000) {
    [void]$sb.AppendLine("Line $i")
}
$result = $sb.ToString()
```

`StringBuilder` maintains a mutable buffer internally, avoiding the cost of repeated allocations and copies. We cast each `AppendLine()` call to `[void]` because `StringBuilder` methods return the instance itself to enable fluent method chaining (e.g., `$sb.Append("A").Append("B")`). Without `[void]`, PowerShell would send the `StringBuilder` object to the output stream on every call, cluttering your results.

### 2.2.2 Here-Strings and Multi-line Text

Here-strings provide a way to define multi-line strings without escaping quotes or newlines. They are delimited by `@"` and `"@` for expandable strings, or `@'` and `'@` for literal strings, and they preserve all formatting—including indentation and line breaks—exactly as typed.

Here-strings are indispensable when you need to embed SQL queries, JSON payloads, XML fragments, or other structured text directly in your script.

#### 2.2.2.1 Syntax Rigidity

Here-strings have strict syntactic rules that trip up many newcomers. The opening delimiter must be followed immediately by a newline, and the closing delimiter must appear on its own line with no leading or trailing whitespace.

**2.2.2.1.1 Newline requirements for opening/closing**

The correct syntax looks like this:

```powershell
$query = @"
SELECT * FROM Users
WHERE LastLogin > '2024-01-01'
"@
```

Note that the opening `@"` is followed immediately by a newline, and the closing `"@` is alone on its line. This format is mandatory:

```powershell
$query = @"SELECT * FROM Users"@   # WRONG: no newline after opening
$query = @"
SELECT * FROM Users
    "@                              # Parse error: closing "@ must be at column 0
```

Both of these will cause parsing errors. The parser uses these delimiters to identify the start and end of the here-string unambiguously.

**2.2.2.1.2 Whitespace sensitivity**

Leading whitespace before the closing delimiter breaks the syntax. If you indent your code for readability, make sure the closing `"@` or `'@` is flush with the left margin or adjust your indentation strategy.

A common workaround is to left-align the closing delimiter even if the rest of your code is indented:

```powershell
function Get-Report {
    $query = @"
SELECT * FROM Logs
WHERE Severity = 'Error'
"@
    Invoke-SqlCmd -Query $query
}
```

This keeps the here-string valid while preserving overall function indentation.

#### 2.2.2.2 Use Cases

Here-strings excel at embedding blocks of text that would otherwise require extensive escaping.

**2.2.2.2.1 Embedding SQL queries**

Rather than building SQL strings line-by-line or escaping every quote, use a here-string:

```powershell
$sql = @"
SELECT u.UserName, u.Email, r.RoleName
FROM Users u
INNER JOIN Roles r ON u.RoleID = r.ID
WHERE u.IsActive = 1
ORDER BY u.UserName
"@

Invoke-SqlCmd -Query $sql -ServerInstance 'localhost'
```

This approach keeps the SQL readable and makes it easy to copy queries to and from SQL Management Studio or other tools.

**2.2.2.2.2 Embedding JSON or XML payloads**

Here-strings are also ideal for REST API calls that require JSON bodies:

```powershell
$json = @"
{
    "name": "Alice",
    "email": "alice@contoso.com",
    "roles": ["Admin", "Developer"]
}
"@

Invoke-RestMethod -Uri 'https://api.contoso.com/users' -Method Post -Body $json -ContentType 'application/json'
```

Because the JSON is defined as a literal block, you avoid quoting and escaping issues that plague concatenated strings.

### 2.2.3 Numeric Types: Int32, Int64, Double, Decimal

PowerShell supports the full range of .NET numeric types, from 8-bit integers to 128-bit decimals. In practice, you will mostly encounter `Int32` (the default for whole numbers), `Double` (the default for floating-point), and occasionally `Decimal` when precision matters. Understanding the differences helps you choose the right type for calculations involving money, scientific data, or performance-critical loops.

The following table compares the most common numeric types:

| Type      | .NET Name         | Size    | Range (approximate)           | Default Literal | Precision Notes                      |
|-----------|-------------------|---------|-------------------------------|-----------------|--------------------------------------|
| `[int]`   | System.Int32      | 32-bit  | ±2.1 billion                  | `42`            | Default for whole numbers            |
| `[long]`  | System.Int64      | 64-bit  | ±9.2 quintillion              | `42L`           | Use for large counts, timestamps     |
| `[double]`| System.Double     | 64-bit  | ±1.7E308                      | `3.14`          | Binary floating-point (IEEE 754)     |
| `[decimal]`| System.Decimal   | 128-bit | ±7.9E28 (28-29 sig. digits)   | `19.99D`        | Base-10, use for currency            |
| `[float]` | System.Single     | 32-bit  | ±3.4E38                       | `3.14f`         | Rarely used; less precise than Double|
| `[byte]`  | System.Byte       | 8-bit   | 0 to 255                      | `[byte]255`     | Unsigned, for binary data            |

#### 2.2.3.1 Type Suffixes

By default, an integer literal like `42` is an `Int32`, and a decimal literal like `3.14` is a `Double`. You can override these defaults with type suffixes or casts.

**2.2.3.1.1 'L' for long, 'D' for decimal**

Append `L` to force a `64`-bit integer:

```powershell
$largeNumber = 9000000000000L
$largeNumber.GetType().FullName   # System.Int64
```

Append `D` to force a `Decimal` type:

```powershell
$precise = 19.99D
$precise.GetType().FullName   # System.Decimal
```

Decimal types are base-10 and maintain precision for financial calculations, whereas Double uses binary floating-point and may introduce rounding errors.

**2.2.3.1.2 Hexadecimal literals (0x...)**

Prefix a number with `0x` to write it in hexadecimal:

```powershell
$mask = 0xFF
$mask   # Outputs 255
```

This is particularly useful when working with bitmasks, color codes, or low-level system programming.

#### 2.2.3.2 Floating Point Precision Issues

Floating-point arithmetic is fast and efficient, but it sacrifices exactness for range and speed. This trade-off is usually acceptable for scientific calculations but can cause subtle bugs in financial or accounting code.

**2.2.3.2.1 IEEE 754 rounding errors in Double**

The Double type uses IEEE 754 binary floating-point, which cannot represent many decimal fractions exactly:

```powershell
1.1 + 2.2   # Outputs 3.3000000000000003
```

This happens because `1.1` and `2.2` have no exact binary representations. The error compounds in arithmetic operations. For most purposes this is harmless, but when exact decimal precision matters—such as currency calculations—you must use `Decimal`.

**2.2.3.2.2 When to strictly use \[decimal\] for finance**

Use `[decimal]` whenever you are dealing with money:

```powershell
[decimal]$price = 19.99
[decimal]$tax = 0.08
[decimal]$total = $price * (1 + $tax)
$total   # Exactly 21.5892, no binary rounding
```

The Decimal type stores values as base-10, ensuring that operations like `0.1 + 0.2` yield exactly `0.3` without floating-point artifacts.

### 2.2.4 Casting and Type Conversion (\[int\], \[string\])

PowerShell performs type conversions automatically in many contexts, but you can also force conversions explicitly using casts. Understanding how the type converter works helps you anticipate when conversions will succeed, when they will fail, and how to handle edge cases gracefully.

#### 2.2.4.1 The PowerShell Type Converter

PowerShell's type converter lives in the `LanguagePrimitives` class and is far more capable than a simple cast. It knows how to convert strings to numbers, objects to strings, arrays to hashtables (in some cases), and even custom types if they implement the right interfaces.

**2.2.4.1.1 LanguagePrimitives.ConvertTo logic**

When you cast `[int]"42"`, PowerShell invokes `LanguagePrimitives.ConvertTo()` under the hood:

```powershell
$stringValue = "42"
$intValue = [int]$stringValue
$intValue.GetType().FullName   # System.Int32
```

The converter tries multiple strategies: it looks for a `Parse` method on the target type, checks for type converters in the Extended Type System, and falls back to constructor overloads. This flexibility is why PowerShell can convert surprisingly complex types with simple casts.

**2.2.4.1.2 Parsing culture-specific formats (Dates/Currency)**

Type conversion respects the current culture settings for date and number formats:

```powershell
[datetime]"12/31/2024"   # Interpreted as MM/dd/yyyy in en-US
[decimal]"1,234.56"      # Comma as thousands separator in en-US
```

If your script runs in different locales, be explicit about formats or use culture-invariant parsing methods from .NET to avoid surprises.

```powershell
[datetime]::Parse('2024-12-31', [cultureinfo]::InvariantCulture)
[decimal]::Parse('1.234,56', [System.Globalization.CultureInfo]::GetCultureInfo('de-DE'))
```

Being explicit about culture makes date and currency parsing predictable in automation and CI environments. (The shorter cast form `[cultureinfo]'de-DE'` also works; the example uses the more explicit .NET call for clarity.)

#### 2.2.4.2 Parse vs. TryParse Patterns

The .NET Framework provides two parsing patterns: `Parse`, which throws an exception on failure, and `TryParse`, which returns a boolean success flag and outputs the parsed value via a reference parameter.

**2.2.4.2.1 Exception avoidance**

Using `TryParse` lets you handle invalid input gracefully:

```powershell
$inputValue = "NotANumber"
$result = 0
if ([int]::TryParse($inputValue, [ref]$result)) {
    Write-Output "Parsed: $result"
} else {
    Write-Warning "Invalid input: $inputValue"
}
```

This pattern avoids the overhead of exception handling and makes your intent clear: you expect some inputs to be invalid and want to handle them explicitly. The `[ref]` accelerator creates a `PSReference` wrapper object that allows .NET methods with `out` or `ref` parameters to write back into the PowerShell variable. This is necessary because PowerShell passes arguments by value by default, so without `[ref]`, the method would have no way to return the parsed result through the parameter.

**2.2.4.2.2 Static method invocation on primitives**

To call static methods like `Parse` or `TryParse`, use the double-colon syntax:

```powershell
$number = [int]::Parse("42")
$date = [datetime]::Parse("2024-01-15")
```

This is more explicit than a cast and gives you access to overloads that accept format providers or parse options.

#### 2.2.4.3 Common Type Accelerators

PowerShell provides type accelerators—short aliases for frequently used .NET types. These make code more readable and reduce typing. The following table lists commonly used accelerators:

| Accelerator    | Full .NET Type                              | Purpose                                   |
|----------------|---------------------------------------------|-------------------------------------------|
| `[int]`        | System.Int32                                | 32-bit integer                            |
| `[long]`       | System.Int64                                | 64-bit integer                            |
| `[string]`     | System.String                               | Text string                               |
| `[char]`       | System.Char                                 | Single character                          |
| `[bool]`       | System.Boolean                              | True/false value                          |
| `[byte]`       | System.Byte                                 | 8-bit unsigned integer                    |
| `[double]`     | System.Double                               | 64-bit floating-point                     |
| `[decimal]`    | System.Decimal                              | 128-bit decimal (for currency)            |
| `[array]`      | System.Array                                | Base array type                           |
| `[xml]`        | System.Xml.XmlDocument                      | XML document                              |
| `[datetime]`   | System.DateTime                             | Date and time                             |
| `[hashtable]`  | System.Collections.Hashtable                | Key-value dictionary                      |
| `[pscustomobject]` | System.Management.Automation.PSCustomObject | Custom object (common in pipelines)   |
| `[psobject]`   | System.Management.Automation.PSObject       | PowerShell object wrapper                 |
| `[regex]`      | System.Text.RegularExpressions.Regex        | Regular expression                        |
| `[scriptblock]`| System.Management.Automation.ScriptBlock    | Executable code block                     |
| `[timespan]`   | System.TimeSpan                             | Time intervals                            |
| `[guid]`       | System.Guid                                 | Unique identifiers                        |
| `[uri]`        | System.Uri                                  | URLs and endpoints                        |
| `[ipaddress]`  | System.Net.IPAddress                        | IP parsing and comparison                 |
| `[version]`    | System.Version                              | Version comparison                        |
| `[ref]`        | System.Management.Automation.PSReference    | Pass-by-reference wrapper                 |

Note: `[ordered]` is sometimes listed alongside type accelerators, but it is **not** a general-purpose type accelerator. It is a special keyword that can only be applied to a hashtable literal (`[ordered]@{}`). You cannot use it to cast an arbitrary variable (e.g., `[ordered]$someVar` will fail). See §2.3.5.2 for details on creating ordered dictionaries.

### 2.2.5 Booleans and Switching Logic

Boolean logic is straightforward in PowerShell, but the language has some convenient shortcuts for evaluating "truthiness" and working with conditionals. Understanding what counts as true or false helps you write cleaner, more idiomatic code.

#### 2.2.5.1 Truthy and Falsy Evaluation

PowerShell follows common conventions for truthiness: zero, empty strings, empty collections, and `$null` are all considered false in a boolean context. Everything else is true.

**2.2.5.1.1 What evaluates to $false (0, empty string, $null)**

The following values are "falsy":

```powershell
if (0) { "True" } else { "False" }             # Outputs "False"
if ("") { "True" } else { "False" }            # Outputs "False"
if ($null) { "True" } else { "False" }         # Outputs "False"
if (@()) { "True" } else { "False" }           # Outputs "False" (empty array)
```

Everything else, including non-zero numbers, non-empty strings, and non-empty collections, evaluates to `$true`. This table summarizes the rules:

| Value Type          | Example           | Boolean Evaluation | Notes                                    |
|---------------------|-------------------|--------------------|------------------------------------------|
| Integer zero        | `0`               | `$false`           | Any numeric zero (0.0, 0L, 0D)           |
| Empty string        | `""`              | `$false`           | Both `""` and `''`                       |
| `$null`             | `$null`           | `$false`           | Absence of a value                       |
| Empty array         | `@()`             | `$false`           | Array with zero elements; however, `@($null)` evaluates to `$true` because it contains one element (even though that element is `$null`) |
| Non-zero number     | `1`, `-5`, `3.14` | `$true`            | Any non-zero numeric value               |
| Non-empty string    | `"text"`, `" "`   | `$true`            | Even whitespace-only strings are truthy  |
| Non-empty array     | `@(1)`, `@($null)`| `$true`            | Array with any elements, even `$null`    |
| Objects             | Any object        | `$true`            | Any instantiated object                  |

**2.2.5.1.2 The \[bool\] cast behavior**

To normalize any value to an explicit boolean, cast it to `[bool]`:

```powershell
[bool]0         # $false
[bool]1         # $true
[bool]""        # $false
[bool]"text"    # $true
```

This is useful when you want to store or return a clear true/false value rather than relying on implicit truthiness.

#### 2.2.5.2 Preview: `$_` / `$PSItem` in `switch`

PowerShell also sets `$_` (alias `$PSItem`) inside `switch` clause script blocks, where it refers to the current input element being evaluated.

We keep the full `switch` syntax and flow-control patterns in Chapter 5 ("Flow Control and Logic") to keep this chapter focused on types. For now, remember the scope rule: `$_` is only meaningful inside the active `switch` clause, so copy it to a normal variable if you need it later.

## 2.3 Arrays and Collections

Arrays and collections are fundamental to any scripting language, but PowerShell's implementation has some surprising performance characteristics. The default array syntax creates fixed-size `System.Array` instances, which are fine for small datasets but perform poorly when you add items in a loop. Understanding when to use arrays, when to use generic lists, and how to work with hashtables and dictionaries will save you from common performance pitfalls.

### 2.3.1 Fixed-size Arrays vs. ArrayLists vs. Generic Lists

The simplest collection in PowerShell is an array, created with `@()` or by assigning multiple values:

```powershell
$array = @(1, 2, 3)
$array = 1, 2, 3   # Equivalent
```

These are fixed-size arrays. Adding or removing elements requires creating a new array, copying the old contents, and discarding the original—a process that becomes prohibitively expensive for large collections. Choosing the right collection type is critical for performance:

| Collection Type                  | Mutability   | Performance (Add) | Type Safety | When to Use                                  |
|----------------------------------|--------------|-------------------|-------------|----------------------------------------------|
| `@()` (Array)                    | Fixed-size   | O(n) per add      | Weak        | Small, known-size collections; pipeline capture |
| `[ArrayList]`                    | Dynamic      | O(1) amortized    | Weak        | Legacy code; backwards compatibility         |
| `[List[T]]`                      | Dynamic      | O(1) amortized    | Strong      | **Recommended**: Building large collections  |
| `[HashSet[T]]`                   | Dynamic      | O(1) average      | Strong      | Unique values; fast membership tests         |
| `[Dictionary[K,V]]`              | Dynamic      | O(1) average      | Strong      | Key-value pairs with strong typing           |

#### 2.3.1.1 The System.Array limitation

PowerShell arrays are instances of `System.Array`, which is fixed in size once created. When you use `+=` to "add" an element, PowerShell invokes the array concatenation operator (`+`), which allocates a new, larger array, copies all existing elements into it, appends the new element, and rebinds the variable to the new array. The old array becomes eligible for garbage collection. This is not an in-place modification—it is a full rebuild through concatenation.

Note that for simple accumulation, you can also capture the output of a `foreach` statement directly into a variable, which lets PowerShell collect results internally without repeated array allocations:

```powershell
$results = foreach ($i in 1..10000) {
    "Item $i"
}
```

This approach can be more concise than using a `List<T>` for straightforward cases where you simply need to gather output.

**2.3.1.1.1 Rebuilding arrays on resize (copy overhead)**

Every `+=` operation on an array triggers a full rebuild:

```powershell
$array = @()
$array += 1
$array += 2
$array += 3
```

This seemingly innocent code creates four array objects in total: the initial empty array, then a new one-element array (copying zero elements), then a new two-element array (copying one element), and finally a new three-element array (copying two elements). Put another way, the `+` operator always produces a new array, so `$array += $x` is equivalent to `$array = $array + $x`—a reassignment to a newly allocated array each time. For small counts this is fast, but as the array grows the cost becomes quadratic.

**2.3.1.1.2 Performance degradation in loops**

A common mistake is to build an array in a loop:

```powershell
$results = @()
foreach ($i in 1..10000) {
    $results += "Item $i"
}
```

This is one of the slowest patterns in PowerShell. By the time you reach element 10,000, you have created 10,000 arrays and performed roughly 50 million copy operations. The script may take minutes when it should take seconds.

#### 2.3.1.2 System.Collections.Generic.List\[T\]

The solution is to use a generic list, which maintains a dynamic internal buffer and grows efficiently:

```powershell
$list = [System.Collections.Generic.List[string]]::new()
foreach ($i in 1..10000) {
    $list.Add("Item $i")
}
```

Note that PowerShell does not have a default type accelerator for `List`, so you must use the full namespace `System.Collections.Generic.List`. To avoid typing this repeatedly, you can import the namespace at the top of your script:

```powershell
using namespace System.Collections.Generic

# Now you can use the short name
$list = [List[string]]::new()
```

In a script file, `using namespace` must appear before executable statements (typically as the first non-comment line).

This pattern completes almost instantly because `List<T>` doubles its internal capacity whenever it runs out of space, ensuring that most additions are O(1) operations.

**2.3.1.2.1 Amortized growth algorithm**

Internally, `List<T>` allocates an array for storage. When you add an element and the array is full, it allocates a new array twice as large, copies the old contents, and discards the old array. This "doubling" strategy ensures that the amortized cost per addition is constant, even though individual resizes are expensive.

**2.3.1.2.2 Type safety benefits**

Generic lists are strongly typed. A `List[int]` can only hold integers:

```powershell
$numbers = [System.Collections.Generic.List[int]]::new()
$numbers.Add(42)
$numbers.Add("text")   # Error: cannot convert string to int
```

This type safety prevents accidental data corruption and enables compiler and runtime optimizations.

### 2.3.2 Array Operators (+, \+=) and Performance Implications

The `+` and `+=` operators work on arrays but have the same copy-on-write semantics as assignment. Using them in loops is the single most common performance mistake in PowerShell scripting.

#### 2.3.2.1 The \+= Antipattern

Avoid `+=` in loops whenever possible:

```powershell
# BAD: O(n^2) complexity
$results = @()
foreach ($item in $data) {
    $results += Process-Item $item
}
```

Every iteration creates a new array. If `$data` has 1,000 elements, this loop performs roughly 500,000 array element copies.

**2.3.2.1.1 Memory allocation tracing**

You can visualize the problem by thinking about memory allocation: iteration 1 allocates an array of size 1, iteration 2 allocates size 2 (copying 1 element), iteration 3 allocates size 3 (copying 2 elements), and so on. The total number of copies is `1 + 2 + 3 + ... + n`, which is O(n²).

**2.3.2.1.2 O(N^2) complexity analysis**

This quadratic complexity means that doubling the size of `$data` quadruples the runtime. Scripts that take seconds with 100 items can take minutes with 1,000 and hours with 10,000. Recognizing `+=` in a loop is often the key to diagnosing slow scripts.

#### 2.3.2.2 Efficient Alternatives

PowerShell provides several idiomatic patterns for collecting results without `+=`.

**2.3.2.2.1 Assignment from pipeline output**

The simplest fix is to let PowerShell collect results automatically:

```powershell
$results = foreach ($item in $data) {
    Process-Item $item
}
```

Here, the `foreach` statement outputs each result to the pipeline, and the assignment operator collects them into an array in a single efficient operation. This is PowerShell's natural, optimized pattern for building collections.

Other idiomatic collection patterns include:

- **Pipeline assignment**: `$results = Get-ChildItem | Where-Object Length -gt 1MB`
- **ForEach-Object with assignment**: `$results = 1..100 | ForEach-Object { $_ * 2 }`
- **Array subexpression**: `$results = @( Get-Process | Select-Object -First 10 )`
- **Method chaining**: `$results = (Get-Process).Where{$_.CPU -gt 1}`

**2.3.2.2.2 Using `List[T].Add()` (or `ArrayList` for Legacy Code)**

If you must build a collection explicitly, use a list:

```powershell
$results = [System.Collections.Generic.List[object]]::new()
foreach ($item in $data) {
    $results.Add((Process-Item $item))
}
```

Or use the older `ArrayList` when you specifically need a non-generic collection (for example, when maintaining legacy scripts that already use it):

```powershell
$results = New-Object 'System.Collections.ArrayList'
foreach ($item in $data) {
    [void]$results.Add((Process-Item $item))
}
```

Both patterns are vastly faster than `+=` for large datasets.

### 2.3.3 Indexing, Slicing, and Range Operators (..)

PowerShell arrays support flexible indexing: you can retrieve individual elements, extract slices, or even select non-contiguous subsets in a single operation.

#### 2.3.3.1 Advanced Indexing

Arrays are zero-indexed:

```powershell
$array = 10, 20, 30, 40
$array[0]   # 10
$array[2]   # 30
```

**2.3.3.1.1 Negative indexing (from end)**

Negative indices count from the end:

```powershell
$array[-1]   # 40 (last element)
$array[-2]   # 30 (second to last)
```

This is a convenient shorthand for accessing tail elements without computing `$array.Count - 1`.

**2.3.3.1.2 Multiple index selection array\[1,3,5\]**

You can retrieve multiple elements by passing an array of indices:

```powershell
$array = 10, 20, 30, 40, 50
$subset = $array[1, 3]   # (20, 40)
```

This pattern is useful when you need to extract a specific set of columns or records from a larger dataset.

#### 2.3.3.2 The Range Operator

The `..` operator generates sequences of integers:

```powershell
1..10   # Outputs 1, 2, 3, ..., 10
```

**2.3.3.2.1 Generating sequences**

Ranges are commonly used in loops:

```powershell
foreach ($i in 1..5) {
    Write-Output "Iteration $i"
}
```

They are also useful for creating test data or padding arrays to a specific size.

**2.3.3.2.2 Reverse ranges (10..1)**

The range operator works in both directions:

```powershell
10..1   # Outputs 10, 9, 8, ..., 1
```

This is a concise way to iterate backward without explicit decrement logic.

### 2.3.4 Multi-dimensional and Jagged Arrays

PowerShell supports both rectangular multi-dimensional arrays (where every row has the same length) and jagged arrays (where rows can have different lengths).

#### 2.3.4.1 Matrix syntax \[,\]

Rectangular arrays use comma-separated dimensions:

```powershell
$matrix = New-Object 'int[,]' 3, 3
$matrix[0, 0] = 1
$matrix[0, 1] = 2
$matrix[1, 0] = 3
```

**2.3.4.1.1 Accessing coordinates \[x,y\]**

Indexing uses the same comma syntax:

```powershell
$value = $matrix[1, 2]
```

**2.3.4.1.2 Initialization limitation**

Creating and initializing rectangular arrays is verbose in PowerShell. There is no concise literal syntax like `@( @(1, 2), @(3, 4) )` for true 2D arrays; that syntax creates jagged arrays instead.

#### 2.3.4.2 Jagged Arrays (Array of Arrays)

Jagged arrays are arrays whose elements are themselves arrays:

```powershell
$jagged = @(
    @(1, 2),
    @(3, 4, 5),
    @(6)
)
```

**2.3.4.2.1 Initialization syntax**

This syntax is natural in PowerShell and mirrors how you would construct such structures in other languages.

**2.3.4.2.2 Accessing via chained brackets \[x\]\[y\]**

To access an element, index the outer array first, then the inner:

```powershell
$jagged[0][1]   # 2 (second element of first sub-array)
$jagged[1][2]   # 5 (third element of second sub-array)
```

Jagged arrays are more common in practice than rectangular arrays because they are easier to create and manipulate.

### 2.3.5 Dictionary and Hashtable Data Structures

Hashtables map keys to values and provide O(1) average-case lookup. They are essential for building lookup tables, grouping data, and passing named parameters via splatting.

#### 2.3.5.1 Hashtable Literals @{}

Create a hashtable with the `@{}` syntax:

```powershell
$config = @{
    Server   = 'localhost'
    Database = 'MyDB'
    Timeout  = 30
}

$config['Server']   # 'localhost'
$config.Database    # 'MyDB' (dot notation also works)
```

**2.3.5.1.1 Key/Value pair storage**

Internally, hashtables use hashing algorithms to map keys to buckets. This makes lookups, inserts, and deletes very fast regardless of the hashtable's size, as long as the hash function distributes keys evenly.

PowerShell hashtables use **case-insensitive** string comparers by default, so `$hash['Key']` and `$hash['key']` return the same value. This case-insensitivity applies only when the keys are strings. If you use keys of other types (e.g., integers, objects), equality is determined by the .NET `Equals()` method of those types. This differs from .NET's `Dictionary<K,V>`, which uses case-sensitive comparisons for string keys by default. If you need case-sensitive string keys, create a hashtable with an explicit comparer or use `[System.Collections.Generic.Dictionary[string,object]]`.

**2.3.5.1.2 Unordered nature of standard Hashtable**

Standard PowerShell hashtables do not preserve insertion order:

```powershell
$hash = @{ A = 1; B = 2; C = 3 }
$hash.Keys   # May output in any order: C, A, B
```

If you need predictable ordering, use an ordered dictionary.

#### 2.3.5.2 Ordered Dictionaries \[ordered\]@{}

The `[ordered]` type accelerator (introduced in PowerShell 3.0 and available in Windows PowerShell 5.1 and PowerShell 7+) creates an `OrderedDictionary` that preserves key insertion order. It is shorthand for the full type name `System.Collections.Specialized.OrderedDictionary`:

```powershell
$ordered = [ordered]@{
    First  = 1
    Second = 2
    Third  = 3
}

$ordered.Keys   # Always outputs: First, Second, Third
```

**2.3.5.2.1 OrderedDictionary internals**

Internally, the ordered dictionary pairs a hash table with a list of keys to preserve insertion order during enumeration.

**2.3.5.2.2 Preserving insertion order for reports**

Ordered dictionaries are particularly useful when creating custom objects for export:

```powershell
$row = [ordered]@{
    Name   = "Alice"
    Age    = 30
    City   = "Seattle"
}

[PSCustomObject]$row | Export-Csv -Path report.csv -NoTypeInformation
```

This ensures that CSV columns appear in the order you specified, rather than in a hash-table-determined order.

## 2.4 Scopes and Lifetime

Variable scope determines where a variable is visible and how long it lives. PowerShell's scoping rules are deliberately hierarchical: child scopes can read from their parents, but writes create local copies unless you explicitly target a parent scope. Understanding these rules is essential for writing functions that encapsulate their state properly and for debugging scripts where variables mysteriously have the wrong value.

Scope is not just an academic concept; it directly affects how modules, functions, and scripts interact with each other and with the global session state. Misunderstanding scope leads to scripts that "work" interactively but fail when packaged as modules, or functions that unexpectedly modify global variables and create hard-to-diagnose bugs.

PowerShell provides several scope modifiers that control variable visibility:

| Scope Modifier | Visibility                     | Lifetime                          | Common Use Case                        |
|----------------|--------------------------------|-----------------------------------|----------------------------------------|
| `global:`      | Entire session                 | Until session ends                | Session-wide configuration, shared state |
| `script:`      | Current script or module       | Until script/module completes     | Script-level helpers, module state     |
| `local:`       | Current scope only (default)   | Until scope exits                 | Function-local variables               |
| `private:`     | Current scope, not children    | Until scope exits                 | Hiding implementation details          |
| `0`, `1`, `2`  | Relative (0=current, 1=parent) | Depends on target scope           | Debugging, advanced scope manipulation |

### 2.4.1 Global, Script, and Local Scopes

PowerShell maintains a stack of scopes. At the bottom is the global scope, which persists for the lifetime of the session. Above that are script scopes (one per script or module being executed) and local scopes (created by functions and by executing script blocks, unless they are dot-sourced into the current scope).

#### 2.4.1.1 Scope Hierarchy

When you reference a variable, PowerShell searches the current scope first, then walks up the runtime call stack (local → script → global) until it finds a match. This is a dynamic lookup: the value you see depends on where a script block is invoked, and module boundaries introduce their own script scopes to prevent accidental leakage.

**2.4.1.1.1 Inheritance of read-access**

Child scopes can read variables from parent scopes:

```powershell
$scriptVar = "I am in the script scope"

function Test-Scope {
    # Reads from the parent (Script) scope
    Write-Output $scriptVar   # Outputs "I am in the script scope"
}

Test-Scope
```

The function sees `$scriptVar` because it inherits read access from its parent scope. When this code runs inside a `.ps1` file, the top-level variable lives in the **Script** scope, not the Global scope. A true Global variable (created with `$global:Var`) persists after the script ends, whereas a Script-scoped variable does not. The key principle is that child scopes can read variables defined in any ancestor scope, regardless of which level that ancestor occupies.

Note that when code is packaged as a **module** (`.psm1`), it gets its own isolated script scope. Variables and functions defined at the top level of a module are not visible to the global session or other modules unless explicitly exported (for example, with `Export-ModuleMember`). This is a common source of confusion when transitioning from scripts to modules.

**2.4.1.1.2 Copy-on-write behavior for child scopes**

If a child scope *assigns* to a variable name that exists in a parent scope, it creates a new local variable rather than modifying the parent:

```powershell
$value = "Parent"

function Modify-Value {
    $value = "Child"
    Write-Output "Inside function: $value"
}

Modify-Value
Write-Output "Outside function: $value"
```

Output:

```text
Inside function: Child
Outside function: Parent
```

The function's assignment created a local `$value` that shadowed the parent variable. The parent `$value` was never changed. This behavior prevents functions from accidentally corrupting their callers' state.

#### 2.4.1.2 The Global Scope

Variables in the global scope persist for the entire PowerShell session. They are accessible from anywhere unless shadowed by local variables of the same name.

**2.4.1.2.1 Persisting across script executions**

Global variables survive across script invocations:

```powershell
$global:counter = 0

function Increment-Counter {
    $global:counter++
}

Increment-Counter
Increment-Counter
$global:counter   # Outputs 2
```

This persistence can be useful for maintaining state between commands, but it also increases the risk of name collisions and hidden dependencies.

**2.4.1.2.2 Risks of pollution**

Over-reliance on global variables makes scripts fragile and hard to test. If multiple scripts or modules use `$global:config`, they can overwrite each other's settings or introduce subtle ordering dependencies. Best practice is to avoid global variables except for truly session-wide concerns like logging or configuration loaded from profiles.

Common problems with global scope include:

- **Name collisions**: Multiple scripts using `$global:config` or `$global:counter` overwrite each other
- **Hidden dependencies**: Scripts work only when run in a specific order
- **Testing difficulties**: Unit tests cannot isolate behavior when globals are involved
- **Debugging challenges**: Tracking down which code modified a global variable
- **Module conflicts**: Two modules using the same global variable name interfere with each other

### 2.4.2 Private and Numbered Scopes

In addition to the named scopes (`global:`, `script:`, `local:`), PowerShell supports private scopes and numbered relative scopes for fine-grained control.

#### 2.4.2.1 Scope Modifiers

You can explicitly target a scope by prefixing a variable name with the scope keyword:

```powershell
$global:myVar = "Global"
$script:myVar = "Script"
$local:myVar = "Local"
```

**2.4.2.1.1 Using $private:var to prevent inheritance**

The `private:` modifier creates a variable that is visible only in the current scope, not in child scopes:

```powershell
function Test-Private {
    $private:secret = "Hidden"
    
    & {
        Write-Output $secret   # Outputs nothing ($null) because $secret is private to the parent scope
    }
}
```

This is useful when you want to ensure that helper functions or nested script blocks cannot see certain variables. (If strict mode is enabled, referencing an undefined variable here can produce an error instead.)

**2.4.2.1.2 Enforcing encapsulation**

Module authors use `private:` to hide implementation details from callers. By marking internal state as private, you prevent external code from depending on or accidentally modifying those variables.

#### 2.4.2.2 Relative Scopes

You can reference scopes numerically: `0` is the current scope, `1` is the parent, `2` is the grandparent, and so on:

```powershell
function Outer {
    $value = "Outer"
    
    function Inner {
        $value = "Inner"
        Write-Output "Scope 0: $value"
        Write-Output "Scope 1: $(Get-Variable -Name value -Scope 1 -ValueOnly)"
    }
    
    Inner
}

Outer
```

Output:

```text
Scope 0: Inner
Scope 1: Outer
```

**2.4.2.2.1 Get-Variable \-Scope 1 (Parent)**

The `-Scope` parameter on `Get-Variable`, `Set-Variable`, and `Remove-Variable` lets you manipulate variables in specific scopes:

```powershell
Get-Variable -Name value -Scope 1
```

This retrieves the variable from the parent scope, bypassing any local shadowing.

**2.4.2.2.2 Stack traversal**

Numbered scopes are particularly useful in complex call stacks where you need to inspect or modify state several levels up. This pattern is common in debugging helpers and logging frameworks that need to access caller context.

### 2.4.3 Dot-Sourcing (.) and Scope Injection

Dot-sourcing is a mechanism for executing a script in the current scope rather than in a new child scope. It effectively "injects" the script's variables and functions into the caller's environment.

#### 2.4.3.1 The Mechanism of Dot-Sourcing

Normally, running a script creates a new script scope, executes the script, and then discards that scope:

```powershell
.\MyScript.ps1   # Runs in a new scope
```

Dot-sourcing prefixes the script path with a dot and space:

```powershell
. .\MyScript.ps1   # Runs in the current scope
```

**2.4.3.1.1 Executing in current scope vs new scope**

When you dot-source a script, any variables or functions it defines remain in the caller's scope after the script completes:

```powershell
# Functions.ps1
function Get-Greeting {
    "Hello, World"
}

# In console:
. .\Functions.ps1
Get-Greeting   # Works: the function is now defined in the current scope
```

Without the dot, the function would disappear as soon as the script finished.

**2.4.3.1.2 Memory implications of keeping variables alive**

Dot-sourced variables persist until you remove them or close the session. Over time, repeatedly dot-sourcing scripts can clutter your session with stale definitions. Use dot-sourcing deliberately, and consider using modules for more formal encapsulation.

#### 2.4.3.2 Library Pattern

A common pattern is to keep helper functions in a separate file and dot-source it at the beginning of your scripts:

```powershell
# Helpers.ps1
function Write-Log {
    param([string]$Message)
    "$([datetime]::Now) - $Message" | Out-File -Append log.txt
}

# MainScript.ps1
. .\Helpers.ps1
Write-Log "Script started"
```

**2.4.3.2.1 Loading utility functions via dot-source**

This pattern keeps your main script focused on business logic while the helpers provide reusable infrastructure. It is a stepping stone toward formal module development.

**2.4.3.2.2 Comparison to Import-Module**

Dot-sourcing is quick and informal but does not provide isolation, versioning, or discoverability. Modules offer a formal structure with manifests, version control, and the ability to unload and reload cleanly. For production tooling, prefer modules; for ad-hoc prototyping or one-off utilities, dot-sourcing is acceptable.

| Feature                | Dot-Sourcing         | Import-Module           |
|------------------------|----------------------|-------------------------|
| Scope isolation        | None (injects)       | Yes (module scope)      |
| Version control        | No                   | Yes (manifests)         |
| Dependency management  | Manual               | Automatic (RequiredModules) |
| Discoverability        | Manual paths         | PSModulePath, galleries |
| Unloading              | Manual removal       | `Remove-Module`         |
| Best for               | Quick prototypes     | Production tools        |

### 2.4.4 Modification via Set-Variable \-Scope

The `Set-Variable` cmdlet lets you bypass copy-on-write and modify variables in parent scopes directly.

#### 2.4.4.1 Bypassing Copy-On-Write

By default, assignments in a function create local variables. To modify a parent variable, use `Set-Variable -Scope`:

```powershell
$counter = 0

function Increment-Counter {
    # Explicitly read the parent scope to avoid accidental local shadowing bugs.
    Set-Variable -Name counter -Value ((Get-Variable -Name counter -Scope 1 -ValueOnly) + 1) -Scope 1
}

Increment-Counter
$counter   # Outputs 1
```

**2.4.4.1.1 Modifying parent variables directly**

The `-Scope 1` parameter tells PowerShell to target the parent scope. The example also reads the parent value explicitly with `Get-Variable -Scope 1 -ValueOnly` so that later edits (such as adding a local `$counter`) do not silently change the logic. This is one way to deliberately modify a parent variable from within a function, but you should usually prefer returning values (or using `[ref]` parameters when appropriate) to avoid hidden coupling.

**2.4.4.1.2 Communicating state upwards**

This pattern is useful when a function needs to signal status or progress to its caller without returning a value through the pipeline. For example, a helper function might increment a shared counter or set a flag indicating that an error occurred.

#### 2.4.4.2 Options parameter

The `Set-Variable` cmdlet supports an `-Option` parameter that lets you mark variables as ReadOnly or Constant:

```powershell
Set-Variable -Name configPath -Value "C:\Config" -Option ReadOnly
$configPath = "C:\Other"   # Error: Variable is ReadOnly
```

**2.4.4.2.1 Constant vs ReadOnly variables**

| Option     | Can be changed with -Force? | Lifetime                       |
|------------|-----------------------------|--------------------------------|
| ReadOnly   | Yes                         | Until removed or session ends  |
| Constant   | No                          | Until session ends (permanent) |

ReadOnly variables provide a safety net but can be overridden if you explicitly use `-Force`. Constant variables cannot be changed under any circumstances, making them true constants.

**2.4.4.2.2 Protecting critical data**

Use ReadOnly or Constant to protect configuration values that should not change during script execution:

```powershell
Set-Variable -Name ApiBaseUrl -Value "https://api.contoso.com" -Option Constant
# $ApiBaseUrl cannot be changed for the rest of the session
```

This prevents accidental overwrites and makes your intent clear to readers of the code. It does **not** hide or encrypt sensitive data, so do not treat `Constant` as a secrets-protection mechanism.

### 2.4.5 Closure in PowerShell

A closure is a script block that captures the state of its surrounding scope at the time it is created. Closures are most useful when you pass a script block to delayed or callback-style code that runs later in the same process/runspace and you want it to "remember" certain variables.

#### 2.4.5.1 GetNewClosure Method

By default, script blocks reference variables from the scope in which they execute, not the scope in which they were defined. The `.GetNewClosure()` method "freezes" the current variable state into the script block:

```powershell
$x = 10
$block = { Write-Output $x }

$x = 20
& $block   # Outputs 20 (uses current value)

$closedBlock = { Write-Output $x }.GetNewClosure()
$x = 30
& $closedBlock   # Outputs 20 (captured value from when GetNewClosure was called)
```

**2.4.5.1.1 Capturing the current state in a ScriptBlock**

`.GetNewClosure()` takes a snapshot of all variables referenced by the script block and embeds them in a private scope. When you later execute the script block, it uses these captured values instead of looking them up in the calling scope.

**2.4.5.1.2 Detaching from dynamic module binding**

Closures also isolate the script block from module-specific state, which is useful when you define event handlers or callbacks inside a module and don't want them to inherit the module's internal variables.

#### 2.4.5.2 Use Cases

Closures are most commonly needed in event-driven or asynchronous programming, especially for callbacks that execute later in the same runspace.

**2.4.5.2.1 Event handlers**

When you register an event handler, the handler may execute long after the current scope has exited. In many cases, `-MessageData` is a simpler way to pass state than capturing variables in a closure:

```powershell
$process = Start-Process notepad -PassThru
$process.EnableRaisingEvents = $true
$message = "Notepad has exited"

# Write-Host is handy for interactive host notifications
Register-ObjectEvent -InputObject $process -EventName Exited -MessageData $message -Action {
    Write-Host $event.MessageData
}
```

Note that the action block of `Register-ObjectEvent` runs as a `PSEventJob` in the same session, but in a separate runspace. `Write-Host` is useful for immediate host-visible notifications in many interactive sessions, while `Write-Output` sends data to the event job's output stream; use `Receive-Job` on the subscriber job when you want predictable pipeline output.

This keeps the event payload explicit and avoids mixing `$using:` and closures in the same example.

Common scenarios where closures are essential:

- **Event handlers**: Capturing context for delayed execution when events fire
- **Deferred script blocks in the same runspace**: Timers, callbacks, or generated functions that run later
- **GUI callbacks**: Button click handlers in WPF or WinForms
- **Async workflows**: Timer callbacks, scheduled tasks
- **Dynamic function generation**: Creating multiple functions that each "remember" specific values

**2.4.5.2.2 Jobs and Out-of-Process State Passing**

Jobs are a different case from same-process callbacks. A `Start-Job` job runs out of process, so pass state explicitly with parameters or `$using:` and remember that data is serialized:

```powershell
# Approach 1: $using: (idiomatic for simple variable capture)
$id = 123
$job = Start-Job -ScriptBlock {
    Get-Process -Id $using:id
}

# Approach 2: -ArgumentList (better for multiple parameters or complex objects)
$id = 123
$job = Start-Job -ScriptBlock {
    param($processId)
    Get-Process -Id $processId
} -ArgumentList $id
```

The `$using:` scope modifier (PowerShell 3.0+) is the idiomatic way to reference local variables inside a job or remote session. It captures the variable's value at the time the job is created, and the variable must exist in the caller's scope. Use `-ArgumentList` with a `param()` block when you need to pass multiple parameters or prefer explicit parameter naming. Both methods serialize the data across the process boundary, so ensure the objects you pass are serializable.

## 2.5 Putting It Together

Now that we have explored variables, types, collections, and scopes, let's see how these concepts combine in a realistic local workflow. Imagine you want to review the busiest processes on your machine and generate a report that flags items worth a closer look.

You might start by defining typed thresholds and export settings in a hashtable with splatting:

<!-- Illustrative -->
```powershell
[double]$cpuThresholdSeconds = 30
[double]$workingSetThresholdMB = 500

$reportParams = @{
    Path              = Join-Path ([System.IO.Path]::GetTempPath()) "ProcessAudit_$(Get-Date -Format 'yyyyMMdd').csv"
    NoTypeInformation = $true
}
```

Next, define a helper function with typed parameters (demonstrating function-local scope) and collect rows efficiently using a generic list to avoid the `+=` antipattern:

<!-- Illustrative -->
```powershell
function New-ProcessAuditRow {
    param(
        [Parameter(Mandatory)]
        [System.Diagnostics.Process]$Process,
        [double]$CpuThresholdSeconds,
        [double]$WorkingSetThresholdMB
    )

    $cpuSeconds = if ($null -ne $Process.CPU) { [double]$Process.CPU } else { 0.0 }
    $workingSetMB = [math]::Round(($Process.WorkingSet64 / 1MB), 2)

    [PSCustomObject]@{
        Name         = $Process.ProcessName
        Id           = $Process.Id
        CpuSeconds   = [math]::Round($cpuSeconds, 2)
        WorkingSetMB = $workingSetMB
        Status       = if ($cpuSeconds -ge $CpuThresholdSeconds -or $workingSetMB -ge $WorkingSetThresholdMB) {
            'Review'
        } else {
            'Normal'
        }
    }
}

$results = [System.Collections.Generic.List[PSCustomObject]]::new()

foreach ($proc in Get-Process | Sort-Object CPU -Descending | Select-Object -First 50) {
    try {
        $results.Add((
            New-ProcessAuditRow -Process $proc `
                -CpuThresholdSeconds $cpuThresholdSeconds `
                -WorkingSetThresholdMB $workingSetThresholdMB
        ))
    } catch {
        Write-Warning "Skipping process $($proc.ProcessName) (Id=$($proc.Id)): $($_.Exception.Message)"
    }
}
```

At this point, `$results` is a strongly typed list of custom objects with consistent properties. You can now filter, summarize, and export it with standard cmdlets:

<!-- Illustrative -->
```powershell
# Group flagged processes and export
$results | Where-Object Status -eq 'Review' |
    Export-Csv @reportParams

# Count processes by status
$results | Group-Object Status |
    Select-Object Name, Count |
    Format-Table -AutoSize
```

Notice how this session uses:

- **Splatting** (`@reportParams`) to keep the export command clean.
- **Generic lists** to avoid quadratic performance when building `$results`.
- **Constrained typing** (`[double]` thresholds and a typed `[System.Diagnostics.Process]` parameter) to keep inputs predictable.
- **Scoped variables** (function-local variables like `$cpuSeconds` and `$workingSetMB`) to keep temporary state contained.
- **String interpolation** in the report path to include the current date.

Each concept from this chapter contributes to a script that is readable, efficient, and maintainable. The same patterns will recur throughout the rest of the book as we build more complex automation and tooling.

### 2.5.1 Key Takeaways

The following table summarizes the most important concepts from this chapter and when to apply them:

| Concept                          | Key Principle                                  | When to Use                                    |
|----------------------------------|------------------------------------------------|------------------------------------------------|
| Variable naming                  | Use consistent casing (camelCase/PascalCase)   | Always; improves readability                   |
| Type constraints                 | Lock variables to types for data integrity     | Parameters, critical config values             |
| Splatting                        | Define parameters in hashtables                | Long parameter lists, conditional parameters   |
| Environment variables            | Use `[Environment]::SetEnvironmentVariable`    | Persistent cross-session configuration         |
| String interpolation             | Use `$()` for expressions in strings           | Building dynamic messages, paths               |
| Here-strings                     | Multi-line text with `@"..."@`                 | SQL queries, JSON/XML payloads                 |
| `[decimal]` for money            | Always use `[decimal]` for currency            | Financial calculations requiring precision     |
| Generic `List[T]`                | Use instead of arrays for dynamic collections  | Building collections in loops                  |
| Avoid `+=` in loops              | Use `List[T].Add()` or pipeline capture        | Always; prevents O(n²) performance             |
| Scope: prefer local              | Avoid global scope except when necessary       | Function-local state, encapsulation            |
| Dot-sourcing                     | Loads scripts into current scope               | Quick prototypes, temporary helper functions   |
| Closures                         | Use `.GetNewClosure()` for deferred execution  | Event handlers, callbacks in the same runspace |

## 2.6 Memory Management and Garbage Collection (Optional Deep Dive)

Because PowerShell runs on the .NET runtime, memory management is handled automatically by the Common Language Runtime's garbage collector (GC). You rarely need to think about when objects are deallocated; the GC continuously monitors object references and reclaims memory when objects become unreachable. The .NET GC is a tracing, generational collector—not reference counting—so it periodically walks the object graph from known “roots” rather than tracking per-object counters.

Understanding the basics of garbage collection helps you write more efficient scripts, especially when you are processing large datasets or running long-lived automation services. While you should not obsess over manual memory management, recognizing when and why the GC runs can help you avoid performance pitfalls.

### 2.6.1 Reachability, generations, and scope

The garbage collector determines reachability by tracing references from "roots"—global variables, local variables on the call stack, and static fields—and promotes frequently used objects into higher generations to avoid frequent scans. If an object can be reached by following references from any root, it is considered live. If no path exists, the object is eligible for collection.

PowerShell's scoping rules interact with this model. When a function exits, its local variables go out of scope, breaking their references to objects. If those objects are not referenced elsewhere, they become eligible for garbage collection.

**2.6.1.1 When objects become eligible for GC**

An object becomes eligible for collection as soon as no reachable references point to it. For example:

```powershell
function Process-Data {
    $largeArray = 1..1000000
    # Use $largeArray
    # When the function returns, $largeArray goes out of scope
}

Process-Data
# The array is now eligible for GC
```

Once the function exits, `$largeArray` is no longer accessible, and the million-element array it referenced can be collected. The exact timing of collection depends on GC heuristics—memory pressure, heap generation, and allocation patterns—but the object is now *eligible* for cleanup.

**2.6.1.2 Removing variables (Remove-Variable) vs setting null**

Setting a variable to `$null` breaks the reference to the object but leaves the variable itself in the variable table:

<!-- Illustrative -->
```powershell
$data = Get-LargeDataset   # Stand-in for any large collection
$data = $null   # The dataset object is now eligible for GC, but $data still exists
```

Using `Remove-Variable` deletes the variable entry entirely:

```powershell
Remove-Variable -Name data
# $data no longer exists in the variable table
```

In most cases, setting `$null` is sufficient to trigger cleanup. Removing the variable is useful when you want to ensure that no code accidentally relies on a variable that should no longer be in scope.

### 2.6.2 Forcing Garbage Collection

In normal operation you should almost never need to manually invoke the garbage collector. The runtime is highly tuned and usually makes better decisions than ad-hoc `Collect()` calls. If you are seeing memory pressure, first look for allocation patterns you can improve (streaming, disposal, and collection choices) before considering a forced collection.

The `[System.GC]::Collect()` method triggers a full garbage collection. Use it sparingly and only when profiling shows a clear benefit.

**2.6.2.1 \[System.GC\]::Collect() scenarios**

Before reaching for `[System.GC]::Collect()`, address memory pressure at the source:

- **Stream instead of materializing**: Prefer pipelines and streaming APIs over loading entire files or datasets into memory at once.
- **Dispose unmanaged resources**: Close or dispose readers, streams, and database objects promptly.
- **Avoid `+=` on arrays**: Repeated array resizing creates unnecessary allocations and GC pressure.
- **Release large references when done**: Setting a large object reference to `$null` can make it eligible for collection sooner.

**Warning:** Forcing garbage collection is almost always harmful to performance. A full `Collect()` call can cause multi-second pauses on large heaps, disrupts the GC's generational self-tuning heuristics, and can temporarily *increase* memory usage as objects are promoted between generations. Never use this in production code without thorough testing under realistic load. If profiling shows a specific memory spike and a forced GC measurably improves a long-running process, treat it as a narrowly scoped diagnostic measure:

```powershell
$data = Import-Csv $path
# Process $data
$data = $null

# Diagnostic only: use after profiling proves it helps
# [System.GC]::Collect()
# [System.GC]::WaitForPendingFinalizers()
```

Even then, a forced GC does not guarantee memory is returned to the OS immediately, and it can reduce throughput by interrupting useful work.

**2.6.2.2 Impact on large object heap (LOH)**

The .NET garbage collector treats objects larger than 85,000 bytes differently, allocating them on the Large Object Heap (LOH). Unlike the regular heap, the LOH is not compacted during collection, which can lead to fragmentation over time.

If your script repeatedly allocates and frees large arrays or strings, consider reusing buffers where possible instead of creating new ones. A forced GC after releasing LOH objects may help in narrowly profiled scenarios, but the real solution is to design algorithms that minimize large allocations in the first place. Do not add explicit GC calls as a generic "memory cleanup" habit: in PowerShell 7+, the runtime's GC defaults are well tuned for most scripts. Newer .NET versions also allow one-time LOH compaction via `GCSettings.LargeObjectHeapCompactionMode`, but that should be a deliberate, profiled choice:

<!-- Tested on: PowerShell 7+ (modern .NET runtime) -->
```powershell
# Diagnostic only – use only after profiling confirms LOH fragmentation is the root cause
[System.Runtime.GCSettings]::LargeObjectHeapCompactionMode = [System.Runtime.GCLargeObjectHeapCompactionMode]::CompactOnce
[System.GC]::Collect()
```

This compacts the LOH only during the next full GC, then reverts to the default mode. It is rarely needed, and even when LOH fragmentation is confirmed by profiling, it should be treated as an exception rather than a routine optimization step.

_Version note: This example is tested on PowerShell 7+ (modern .NET). Windows PowerShell 5.1 environments may behave differently depending on the installed .NET Framework version, so validate support and performance impact before using it._

